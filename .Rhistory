weighted.inputs2 <- c()
for(i in 1:90)
if(w[i]!=0)
{
weighted.inputs2=c(weighted.inputs2, input.weighted.scaled[i])
}
result <- final.trained[final.trained[,1]==knn.predict.input,]
result <- result[,-c(1,8,18,19,20,24:41)]
print(head(weighted.scaled.vars))
print(head(input.weighted.scaled))
View(result)
}
weighted.scaled.vars <- data.frame(1:2000)
scaled.vars.new <- data.frame(1:2000)
for(i in 1:90){
if(w[i]!=0)
weighted.scaled.vars<-cbind(weighted.scaled.vars, scaled.vars[1:2000,i]*sqrt(w[i]))
scaled.vars.new<-cbind(scaled.vars.new, scaled.vars[1:2000,i])
}
w[i]
scaled.vars[1:2000,i]
df <- read.csv("/Users/Gaurab Bose/Desktop/InvestAI/data/ETFdb.csv", header=TRUE)
library(wskm)
library(cluster)
library(dummies)
library(caret)
#clean data for NA
set.seed(1)
#relevant vars
vars <- df[,c(1,3,4,6,11,12,13,14,16,21,22,38)]
#deal with %
vars$YTD<-as.numeric(sub("%","",vars$YTD))
vars$X1.Year<-as.numeric(sub("%","",vars$X1.Year))
vars$X3.Year<-as.numeric(sub("%","",vars$X3.Year))
vars$X5.Year<-as.numeric(sub("%","",vars$X5.Year))
vars$ETFdb.Category=as.factor(vars$ETFdb.Category)
vars<-cbind(vars, dummy(vars$ETFdb.Category))
vars<-cbind(vars, dummy(vars$Volatility))
vars<-vars[,-8]
vars<-vars[,-11]
vars<-vars[,-1]
vars$ER <-as.numeric(sub("%","",vars$ER))
vars[,8]<-as.numeric(sub("%","",vars[,8]))
vars[vars$P.E.Ratio=="n/a",9]=NA
vars[,9]=as.numeric(vars[,9])
vars<-vars[,-2]
#when changing code for data. Change 1) 1200, number of vars here,
#scale and standadize
#calc means of vars
means=c()
for(i in 1:90)
vars[1:2000, i]=as.numeric(vars[1:2000,i])
for(i in 1:90)
means[i]=mean(vars[1:2000,i], na.rm=TRUE)
sds=c()
for(i in 1:90)
sds[i]=sd(vars[1:2000,i], na.rm=TRUE)
scaled.vars <- data.frame(scale(vars[,1:8]))
scaled.vars <- cbind(scaled.vars, vars[,9:90])
weight.denom = 3
#need to add weights here
calc <- function(inputs, w){
weighted.scaled.vars <- data.frame(1:2000)
scaled.vars.new <- data.frame(1:2000)
for(i in 1:90){
if(w[i]!=0){
weighted.scaled.vars<-cbind(weighted.scaled.vars, scaled.vars[1:2000,i]*sqrt(w[i]))
scaled.vars.new<-cbind(scaled.vars.new, scaled.vars[1:2000,i])
}
}
weighted.scaled.vars = weighted.scaled.vars[,-1]
scaled.vars.new = scaled.vars.new[,-1]
weighted.scaled.vars[is.na(weighted.scaled.vars)] <- 0
scaled.vars.new[is.na(scaled.vars.new)] <- 0
model.kmeans <- kmeans(weighted.scaled.vars, 100)
final.trained <- data.frame(model.kmeans$cluster, df[1:2000,]) #can change no of clusters
final.trained.vars <- data.frame(model.kmeans$cluster, vars[1:2000,]) #can change no of clusters
#INPUTS
#===============
input.weighted.scaled.new=c()
input.weighted.scaled = append((input.vals[1:8]-means[1:8])/sds[1:8], input.vals[9:90])
for(i in 1:90){
if(w[i]!=0)
input.weighted.scaled.new=c(input.weighted.scaled.new, input.weighted.scaled[i])
}
input.frame <- t(data.frame(input.weighted.scaled.new))
print("hello")
print(input.frame)
print("hello")
#predict bucket
knn.predict.input <- knn(scaled.vars.new, input.frame, final.trained[1:2000,1], k=1)
weighted.inputs2 <- c()
for(i in 1:90)
if(w[i]!=0)
{
weighted.inputs2=c(weighted.inputs2, input.weighted.scaled[i])
}
result <- final.trained[final.trained[,1]==knn.predict.input,]
result <- result[,-c(1,8,18,19,20,24:41)]
print(head(weighted.scaled.vars))
print(head(input.weighted.scaled))
View(result)
}
#weights
w=c(1,1000,rep(0,88))
input.vals = c(10,-7, rep(0,88))
calc(input.vals, w)
# what needs to be manipulated - weights (how big should they be 10s or 1000s.) AND refine number of clusters
# give the cluster no. var a name - then increment it. do the whole calc, and see where you get two similar ones in the same place
# all NA are replaced with 0 - affects results
# end ETF sub
# SPECIFY - IDEALLY CHOOSE 3-4 parameters, the lesser you choose, the easier it is to find a good match in the universe of existing ETFs
# SHOW KNN SCORE HOW CLOSE A MATCH IT IS
# INCORPORATE WEIGHTING
# INCORPORATE equities
df <- read.csv("/Users/Gaurab Bose/Desktop/InvestAI/data/ETFdb.csv", header=TRUE)
library(wskm)
library(cluster)
library(dummies)
library(caret)
#clean data for NA
set.seed(1)
#relevant vars
vars <- df[,c(1,3,4,6,11,12,13,14,16,21,22,38)]
#deal with %
vars$YTD<-as.numeric(sub("%","",vars$YTD))
vars$X1.Year<-as.numeric(sub("%","",vars$X1.Year))
vars$X3.Year<-as.numeric(sub("%","",vars$X3.Year))
vars$X5.Year<-as.numeric(sub("%","",vars$X5.Year))
vars$ETFdb.Category=as.factor(vars$ETFdb.Category)
vars<-cbind(vars, dummy(vars$ETFdb.Category))
vars<-cbind(vars, dummy(vars$Volatility))
vars<-vars[,-8]
vars<-vars[,-11]
vars<-vars[,-1]
vars$ER <-as.numeric(sub("%","",vars$ER))
vars[,8]<-as.numeric(sub("%","",vars[,8]))
vars[vars$P.E.Ratio=="n/a",9]=NA
vars[,9]=as.numeric(vars[,9])
vars<-vars[,-2]
#when changing code for data. Change 1) 1200, number of vars here,
#scale and standadize
#calc means of vars
means=c()
for(i in 1:90)
vars[1:2000, i]=as.numeric(vars[1:2000,i])
for(i in 1:90)
means[i]=mean(vars[1:2000,i], na.rm=TRUE)
sds=c()
for(i in 1:90)
sds[i]=sd(vars[1:2000,i], na.rm=TRUE)
scaled.vars <- data.frame(scale(vars[,1:8]))
scaled.vars <- cbind(scaled.vars, vars[,9:90])
weight.denom = 3
#need to add weights here
calc <- function(inputs, w){
weighted.scaled.vars <- data.frame(1:2000)
scaled.vars.new <- data.frame(1:2000)
for(i in 1:90){
if(w[i]!=0){
weighted.scaled.vars<-cbind(weighted.scaled.vars, scaled.vars[1:2000,i]*sqrt(w[i]))
scaled.vars.new<-cbind(scaled.vars.new, scaled.vars[1:2000,i])
}
}
weighted.scaled.vars = weighted.scaled.vars[,-1]
scaled.vars.new = scaled.vars.new[,-1]
weighted.scaled.vars[is.na(weighted.scaled.vars)] <- 0
scaled.vars.new[is.na(scaled.vars.new)] <- 0
model.kmeans <- kmeans(weighted.scaled.vars, 100)
final.trained <- data.frame(model.kmeans$cluster, df[1:2000,]) #can change no of clusters
final.trained.vars <- data.frame(model.kmeans$cluster, vars[1:2000,]) #can change no of clusters
#INPUTS
#===============
input.weighted.scaled.new=c()
input.weighted.scaled = append((input.vals[1:8]-means[1:8])/sds[1:8], input.vals[9:90])
for(i in 1:90){
if(w[i]!=0)
input.weighted.scaled.new=c(input.weighted.scaled.new, input.weighted.scaled[i])
}
input.frame <- t(data.frame(input.weighted.scaled.new))
print("hello")
print(input.frame)
print("hello")
#predict bucket
knn.predict.input <- knn(scaled.vars.new, input.frame, final.trained[1:2000,1], k=1)
weighted.inputs2 <- c()
for(i in 1:90)
if(w[i]!=0)
{
weighted.inputs2=c(weighted.inputs2, input.weighted.scaled[i])
}
result <- final.trained[final.trained[,1]==knn.predict.input,]
result <- result[,-c(1,8,18,19,20,24:41)]
print(head(weighted.scaled.vars))
print(head(input.weighted.scaled))
View(result)
}
#weights
w=c(1000,1,rep(0,88))
input.vals = c(10,-7, rep(0,88))
calc(input.vals, w)
# what needs to be manipulated - weights (how big should they be 10s or 1000s.) AND refine number of clusters
# give the cluster no. var a name - then increment it. do the whole calc, and see where you get two similar ones in the same place
# all NA are replaced with 0 - affects results
# end ETF sub
# SPECIFY - IDEALLY CHOOSE 3-4 parameters, the lesser you choose, the easier it is to find a good match in the universe of existing ETFs
# SHOW KNN SCORE HOW CLOSE A MATCH IT IS
# INCORPORATE WEIGHTING
# INCORPORATE equities
df <- read.csv("/Users/Gaurab Bose/Desktop/InvestAI/data/ETFdb.csv", header=TRUE)
library(wskm)
library(cluster)
library(dummies)
library(caret)
#clean data for NA
set.seed(1)
#relevant vars
vars <- df[,c(1,3,4,6,11,12,13,14,16,21,22,38)]
#deal with %
vars$YTD<-as.numeric(sub("%","",vars$YTD))
vars$X1.Year<-as.numeric(sub("%","",vars$X1.Year))
vars$X3.Year<-as.numeric(sub("%","",vars$X3.Year))
vars$X5.Year<-as.numeric(sub("%","",vars$X5.Year))
vars$ETFdb.Category=as.factor(vars$ETFdb.Category)
vars<-cbind(vars, dummy(vars$ETFdb.Category))
vars<-cbind(vars, dummy(vars$Volatility))
vars<-vars[,-8]
vars<-vars[,-11]
vars<-vars[,-1]
vars$ER <-as.numeric(sub("%","",vars$ER))
vars[,8]<-as.numeric(sub("%","",vars[,8]))
vars[vars$P.E.Ratio=="n/a",9]=NA
vars[,9]=as.numeric(vars[,9])
vars<-vars[,-2]
#when changing code for data. Change 1) 1200, number of vars here,
#scale and standadize
#calc means of vars
means=c()
for(i in 1:90)
vars[1:2000, i]=as.numeric(vars[1:2000,i])
for(i in 1:90)
means[i]=mean(vars[1:2000,i], na.rm=TRUE)
sds=c()
for(i in 1:90)
sds[i]=sd(vars[1:2000,i], na.rm=TRUE)
scaled.vars <- data.frame(scale(vars[,1:8]))
scaled.vars <- cbind(scaled.vars, vars[,9:90])
weight.denom = 3
#need to add weights here
calc <- function(inputs, w){
weighted.scaled.vars <- data.frame(1:2000)
scaled.vars.new <- data.frame(1:2000)
for(i in 1:90){
if(w[i]!=0){
weighted.scaled.vars<-cbind(weighted.scaled.vars, scaled.vars[1:2000,i]*sqrt(w[i]))
scaled.vars.new<-cbind(scaled.vars.new, scaled.vars[1:2000,i])
}
}
weighted.scaled.vars = weighted.scaled.vars[,-1]
scaled.vars.new = scaled.vars.new[,-1]
weighted.scaled.vars[is.na(weighted.scaled.vars)] <- 0
scaled.vars.new[is.na(scaled.vars.new)] <- 0
model.kmeans <- kmeans(weighted.scaled.vars, 100)
final.trained <- data.frame(model.kmeans$cluster, df[1:2000,]) #can change no of clusters
final.trained.vars <- data.frame(model.kmeans$cluster, vars[1:2000,]) #can change no of clusters
#INPUTS
#===============
input.weighted.scaled.new=c()
input.weighted.scaled = append((input.vals[1:8]-means[1:8])/sds[1:8], input.vals[9:90])
for(i in 1:90){
if(w[i]!=0)
input.weighted.scaled.new=c(input.weighted.scaled.new, input.weighted.scaled[i])
}
input.frame <- t(data.frame(input.weighted.scaled.new))
print("hello")
print(input.frame)
print("hello")
#predict bucket
knn.predict.input <- knn(scaled.vars.new, input.frame, final.trained[1:2000,1], k=1)
weighted.inputs2 <- c()
for(i in 1:90)
if(w[i]!=0)
{
weighted.inputs2=c(weighted.inputs2, input.weighted.scaled[i])
}
result <- final.trained[final.trained[,1]==knn.predict.input,]
result <- result[,-c(1,8,18,19,20,24:41)]
print(head(weighted.scaled.vars))
print(head(input.weighted.scaled))
View(result)
}
#weights
w=c(1000,1000,rep(0,88))
input.vals = c(10,-7, rep(0,88))
calc(input.vals, w)
# what needs to be manipulated - weights (how big should they be 10s or 1000s.) AND refine number of clusters
# give the cluster no. var a name - then increment it. do the whole calc, and see where you get two similar ones in the same place
# all NA are replaced with 0 - affects results
# end ETF sub
# SPECIFY - IDEALLY CHOOSE 3-4 parameters, the lesser you choose, the easier it is to find a good match in the universe of existing ETFs
# SHOW KNN SCORE HOW CLOSE A MATCH IT IS
# INCORPORATE WEIGHTING
# INCORPORATE equities
df <- read.csv("/Users/Gaurab Bose/Desktop/InvestAI/data/ETFdb.csv", header=TRUE)
library(wskm)
library(cluster)
library(dummies)
library(caret)
#clean data for NA
set.seed(1)
#relevant vars
vars <- df[,c(1,3,4,6,11,12,13,14,16,21,22,38)]
#deal with %
vars$YTD<-as.numeric(sub("%","",vars$YTD))
vars$X1.Year<-as.numeric(sub("%","",vars$X1.Year))
vars$X3.Year<-as.numeric(sub("%","",vars$X3.Year))
vars$X5.Year<-as.numeric(sub("%","",vars$X5.Year))
vars$ETFdb.Category=as.factor(vars$ETFdb.Category)
vars<-cbind(vars, dummy(vars$ETFdb.Category))
vars<-cbind(vars, dummy(vars$Volatility))
vars<-vars[,-8]
vars<-vars[,-11]
vars<-vars[,-1]
vars$ER <-as.numeric(sub("%","",vars$ER))
vars[,8]<-as.numeric(sub("%","",vars[,8]))
vars[vars$P.E.Ratio=="n/a",9]=NA
vars[,9]=as.numeric(vars[,9])
vars<-vars[,-2]
#when changing code for data. Change 1) 1200, number of vars here,
#scale and standadize
#calc means of vars
means=c()
for(i in 1:90)
vars[1:2000, i]=as.numeric(vars[1:2000,i])
for(i in 1:90)
means[i]=mean(vars[1:2000,i], na.rm=TRUE)
sds=c()
for(i in 1:90)
sds[i]=sd(vars[1:2000,i], na.rm=TRUE)
scaled.vars <- data.frame(scale(vars[,1:8]))
scaled.vars <- cbind(scaled.vars, vars[,9:90])
weight.denom = 3
#need to add weights here
calc <- function(inputs, w){
weighted.scaled.vars <- data.frame(1:2000)
scaled.vars.new <- data.frame(1:2000)
for(i in 1:90){
if(w[i]!=0){
weighted.scaled.vars<-cbind(weighted.scaled.vars, scaled.vars[1:2000,i]*sqrt(w[i]))
scaled.vars.new<-cbind(scaled.vars.new, scaled.vars[1:2000,i])
}
}
weighted.scaled.vars = weighted.scaled.vars[,-1]
scaled.vars.new = scaled.vars.new[,-1]
weighted.scaled.vars[is.na(weighted.scaled.vars)] <- 0
scaled.vars.new[is.na(scaled.vars.new)] <- 0
model.kmeans <- kmeans(weighted.scaled.vars, k)
final.trained <- data.frame(model.kmeans$cluster, df[1:2000,]) #can change no of clusters
final.trained.vars <- data.frame(model.kmeans$cluster, vars[1:2000,]) #can change no of clusters
#INPUTS
#===============
input.weighted.scaled.new=c()
input.weighted.scaled = append((input.vals[1:8]-means[1:8])/sds[1:8], input.vals[9:90])
for(i in 1:90){
if(w[i]!=0)
input.weighted.scaled.new=c(input.weighted.scaled.new, input.weighted.scaled[i])
}
input.frame <- t(data.frame(input.weighted.scaled.new))
print("hello")
print(input.frame)
print("hello")
#predict bucket
knn.predict.input <- knn(scaled.vars.new, input.frame, final.trained[1:2000,1], k=1)
weighted.inputs2 <- c()
for(i in 1:90)
if(w[i]!=0)
{
weighted.inputs2=c(weighted.inputs2, input.weighted.scaled[i])
}
result <- final.trained[final.trained[,1]==knn.predict.input,]
result <- result[,-c(1,8,18,19,20,24:41)]
print(head(weighted.scaled.vars))
print(head(input.weighted.scaled))
View(result)
}
#weights
w=c(1000,1000,rep(0,88))
k=50
input.vals = c(10,-7, rep(0,88))
calc(input.vals, w)
# what needs to be manipulated - weights (how big should they be 10s or 1000s.) AND refine number of clusters
# give the cluster no. var a name - then increment it. do the whole calc, and see where you get two similar ones in the same place
# all NA are replaced with 0 - affects results
# end ETF sub
# SPECIFY - IDEALLY CHOOSE 3-4 parameters, the lesser you choose, the easier it is to find a good match in the universe of existing ETFs
# SHOW KNN SCORE HOW CLOSE A MATCH IT IS
# INCORPORATE WEIGHTING
# INCORPORATE equities
df <- read.csv("/Users/Gaurab Bose/Desktop/InvestAI/data/ETFdb.csv", header=TRUE)
library(wskm)
library(cluster)
library(dummies)
library(caret)
#clean data for NA
set.seed(1)
#relevant vars
vars <- df[,c(1,3,4,6,11,12,13,14,16,21,22,38)]
#deal with %
vars$YTD<-as.numeric(sub("%","",vars$YTD))
vars$X1.Year<-as.numeric(sub("%","",vars$X1.Year))
vars$X3.Year<-as.numeric(sub("%","",vars$X3.Year))
vars$X5.Year<-as.numeric(sub("%","",vars$X5.Year))
vars$ETFdb.Category=as.factor(vars$ETFdb.Category)
vars<-cbind(vars, dummy(vars$ETFdb.Category))
vars<-cbind(vars, dummy(vars$Volatility))
vars<-vars[,-8]
vars<-vars[,-11]
vars<-vars[,-1]
vars$ER <-as.numeric(sub("%","",vars$ER))
vars[,8]<-as.numeric(sub("%","",vars[,8]))
vars[vars$P.E.Ratio=="n/a",9]=NA
vars[,9]=as.numeric(vars[,9])
vars<-vars[,-2]
#when changing code for data. Change 1) 1200, number of vars here,
#scale and standadize
#calc means of vars
means=c()
for(i in 1:90)
vars[1:2000, i]=as.numeric(vars[1:2000,i])
for(i in 1:90)
means[i]=mean(vars[1:2000,i], na.rm=TRUE)
sds=c()
for(i in 1:90)
sds[i]=sd(vars[1:2000,i], na.rm=TRUE)
scaled.vars <- data.frame(scale(vars[,1:8]))
scaled.vars <- cbind(scaled.vars, vars[,9:90])
weight.denom = 3
#need to add weights here
calc <- function(inputs, w){
weighted.scaled.vars <- data.frame(1:2000)
scaled.vars.new <- data.frame(1:2000)
for(i in 1:90){
if(w[i]!=0){
weighted.scaled.vars<-cbind(weighted.scaled.vars, scaled.vars[1:2000,i]*sqrt(w[i]))
scaled.vars.new<-cbind(scaled.vars.new, scaled.vars[1:2000,i])
}
}
weighted.scaled.vars = weighted.scaled.vars[,-1]
scaled.vars.new = scaled.vars.new[,-1]
weighted.scaled.vars[is.na(weighted.scaled.vars)] <- 0
scaled.vars.new[is.na(scaled.vars.new)] <- 0
model.kmeans <- kmeans(weighted.scaled.vars, k)
final.trained <- data.frame(model.kmeans$cluster, df[1:2000,]) #can change no of clusters
final.trained.vars <- data.frame(model.kmeans$cluster, vars[1:2000,]) #can change no of clusters
#INPUTS
#===============
input.weighted.scaled.new=c()
input.weighted.scaled = append((input.vals[1:8]-means[1:8])/sds[1:8], input.vals[9:90])
for(i in 1:90){
if(w[i]!=0)
input.weighted.scaled.new=c(input.weighted.scaled.new, input.weighted.scaled[i])
}
input.frame <- t(data.frame(input.weighted.scaled.new))
print("hello")
print(input.frame)
print("hello")
#predict bucket
knn.predict.input <- knn(scaled.vars.new, input.frame, final.trained[1:2000,1], k=1)
weighted.inputs2 <- c()
for(i in 1:90)
if(w[i]!=0)
{
weighted.inputs2=c(weighted.inputs2, input.weighted.scaled[i])
}
result <- final.trained[final.trained[,1]==knn.predict.input,]
result <- result[,-c(1,8,18,19,20,24:41)]
print(head(weighted.scaled.vars))
print(head(input.weighted.scaled))
View(result)
}
#weights
w=c(1000,1000,rep(0,88))
k=200
input.vals = c(10,-7, rep(0,88))
calc(input.vals, w)
# what needs to be manipulated - weights (how big should they be 10s or 1000s.) AND refine number of clusters
# give the cluster no. var a name - then increment it. do the whole calc, and see where you get two similar ones in the same place
# all NA are replaced with 0 - affects results
# end ETF sub
# SPECIFY - IDEALLY CHOOSE 3-4 parameters, the lesser you choose, the easier it is to find a good match in the universe of existing ETFs
# SHOW KNN SCORE HOW CLOSE A MATCH IT IS
# INCORPORATE WEIGHTING
# INCORPORATE equities
shiny::runApp('C:/Users/Gaurab Bose/Desktop/InvestAI')
runApp('C:/Users/Gaurab Bose/Desktop/InvestAI')
install.packages("caret")
install.packages("caret")
shiny::runApp('C:/Users/Gaurab Bose/Desktop/InvestAI')
runApp('C:/Users/Gaurab Bose/Desktop/InvestAI')
shiny::runApp()
shiny::runApp()
runApp()
library(cluster)
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
